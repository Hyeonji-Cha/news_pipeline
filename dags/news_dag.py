from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from datetime import datetime, timedelta
import requests
import psycopg2
import pandas as pd
import re
import os
import sys,os
sys.path.append(os.path.join(os.path.dirname(__file__), "..", "utils"))

from nlp_utils import preprocess_text

def tokenize_news(**kwargs):
    conn = psycopg2.connect(
        host=os.getenv("DB_HOST", "localhost"),
        dbname=os.getenv("DB_NAME", "postgres"),
        user=os.getenv("DB_USER", "postgres"),
        password=os.getenv("DB_PASSWORD", ""),
        port=int(os.getenv("DB_PORT", "5432")),
    )
    cur = conn.cursor()

    # ì „ì²˜ë¦¬ ê²°ê³¼ ì €ìž¥ìš© í…Œì´ë¸”
    cur.execute("""
        CREATE TABLE IF NOT EXISTS news_preprocessed (
            news_id      INTEGER PRIMARY KEY,
            title_clean  TEXT,
            content_clean TEXT,
            updated_at   TIMESTAMPTZ NOT NULL DEFAULT now()
        );
    """)

    # ì›ë³¸ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    cur.execute("SELECT id, title, content FROM news;")
    rows = cur.fetchall()

    # ì „ì²˜ë¦¬ & UPSERT
    for news_id, title, content in rows:
        title_clean   = " ".join(preprocess_text(title or ""))
        content_clean = " ".join(preprocess_text(content or ""))
        cur.execute("""
            INSERT INTO news_preprocessed (news_id, title_clean, content_clean, updated_at)
            VALUES (%s, %s, %s, now())
            ON CONFLICT (news_id) DO UPDATE
            SET title_clean = EXCLUDED.title_clean,
                content_clean = EXCLUDED.content_clean,
                updated_at = now();
        """, (news_id, title_clean, content_clean))

    conn.commit()
    cur.close()
    conn.close()

# ì´ì‚¬ ëŒ€ë¹„: .env ì§€ì› (ì—†ì–´ë„ ë™ìž‘í•˜ë„ë¡ ê¸°ë³¸ê°’ ìœ ì§€)
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# ---- ì´ì‚¬ ëŒ€ë¹„: í™˜ê²½ ë³€ìˆ˜(ì—†ìœ¼ë©´ ê¸°ì¡´ ê°’ ì‚¬ìš©) ----
DATA_DIR = os.getenv("DATA_DIR", "/tmp")
NEWS_API_KEY = os.getenv("NEWS_API_KEY", "")

DB_HOST = os.getenv("DB_HOST", "localhost")
DB_NAME = os.getenv("DB_NAME", "postgres")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASSWORD = os.getenv("DB_PASSWORD", "")
DB_PORT = int(os.getenv("DB_PORT", "5432"))

# ðŸ“¢ Slack ì‹¤íŒ¨ ì•ŒëžŒ ì½œë°±
def slack_alert(context):
    slack_msg = f"""
    :red_circle: Task Failed!
    *DAG*: {context.get('dag').dag_id}
    *Task*: {context.get('task_instance').task_id}
    *Execution Time*: {context.get('execution_date')}
    *Log Url*: {context.get('task_instance').log_url}
    *Exception*: {context.get('exception')}
    """
    alert = SlackWebhookOperator(
        task_id="slack_notification_fail",
        slack_webhook_conn_id="slack_default",
        message=slack_msg,
        channel="#airflow-alert",
        username="airflow-bot",
    )
    return alert.execute(context=context)

# ðŸ“¢ Slack ì„±ê³µ ì•ŒëžŒ ì½œë°± (DAG ë ˆë²¨)
def dag_success_alert(context):
    slack_msg = f"""
    :large_blue_circle: DAG Succeed!
    *DAG*: {context.get('dag').dag_id}
    *Run Id*: {context.get('run_id')}
    *Execution Time*: {context.get('execution_date')}
    """
    alert = SlackWebhookOperator(
        task_id="slack_notification_success",
        slack_webhook_conn_id="slack_default",
        message=slack_msg,
        channel="#airflow-alert",
        username="airflow-bot",
    )
    return alert.execute(context=context)

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 0,
    "retry_delay": timedelta(minutes=5),
    "email_on_failure": False,
    "email_on_retry": False,
    "on_failure_callback": slack_alert,
}

# 1ï¸âƒ£ ë‰´ìŠ¤ API í˜¸ì¶œ
def fetch_news(**kwargs):
    url = "https://newsapi.org/v2/everything"
    params = {
        "q": "AI OR ì¸ê³µì§€ëŠ¥ OR ë¨¸ì‹ ëŸ¬ë‹",
        "pageSize": 50,
        "apiKey": NEWS_API_KEY,
    }
    res = requests.get(url, params=params, timeout=30)
    res.raise_for_status()
    articles = res.json().get("articles", [])
    kwargs["ti"].xcom_push(key="articles", value=articles)

# 2ï¸âƒ£ ì „ì²˜ë¦¬ + CSV ì €ìž¥
def preprocess_news(**kwargs):
    articles = kwargs["ti"].xcom_pull(key="articles", task_ids="fetch_news")
    if not articles:
        raise Exception("No articles fetched for preprocessing!")

    df = pd.DataFrame(articles)
    df = df[["title", "content", "publishedAt"]].dropna()

    def _clean(s: str) -> str:
        s = s.lower()
        s = re.sub(r"[^a-zA-Z0-9ê°€-íž£ ]", " ", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    df["title"] = df["title"].astype(str).apply(_clean)
    df["content"] = df["content"].astype(str).apply(_clean)

    os.makedirs(DATA_DIR, exist_ok=True)
    output_path = os.path.join(DATA_DIR, "preprocessed_news.csv")
    df.to_csv(output_path, index=False)

    kwargs["ti"].xcom_push(key="csv_path", value=output_path)

# 3ï¸âƒ£ CSV â†’ Postgres ì ìž¬ (ì¤‘ë³µ ë°©ì§€: title UNIQUE)
def save_to_postgres(**kwargs):
    csv_path = kwargs["ti"].xcom_pull(key="csv_path", task_ids="preprocess_news")
    if not csv_path or not os.path.exists(csv_path):
        raise Exception("CSV file not found!")

    df = pd.read_csv(csv_path)

    conn = psycopg2.connect(
        host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, port=DB_PORT
    )
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS news (
            id SERIAL PRIMARY KEY,
            title TEXT UNIQUE,
            content TEXT,
            published_at TIMESTAMP
        );
    """)

    for _, row in df.iterrows():
        cur.execute("""
            INSERT INTO news (title, content, published_at)
            VALUES (%s, %s, %s)
            ON CONFLICT (title) DO NOTHING;
        """, (row["title"], row["content"], row["publishedAt"]))

    conn.commit()
    cur.close()
    conn.close()

# 4ï¸âƒ£ ì¼ë³„ ì§‘ê³„ í…Œì´ë¸” ì ìž¬ (AI/ì¸ê³µì§€ëŠ¥/ë¨¸ì‹ ëŸ¬ë‹ í‚¤ì›Œë“œ ì¹´ìš´íŠ¸)
def compute_daily_stats(**kwargs):
    conn = psycopg2.connect(
        host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD, port=DB_PORT
    )
    cur = conn.cursor()

    # ì§‘ê³„ í…Œì´ë¸”
    cur.execute("""
        CREATE TABLE IF NOT EXISTS news_daily_stats (
            stat_date DATE PRIMARY KEY,
            article_count INTEGER NOT NULL,
            ai_count INTEGER NOT NULL,
            ml_count INTEGER NOT NULL,
            ko_ai_count INTEGER NOT NULL,
            updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
        );
    """)

    # ë‰´ìŠ¤ í…Œì´ë¸”ì—ì„œ ì¼ìžë³„ë¡œ ì§‘ê³„í•´ì„œ upsert
    cur.execute("""
        WITH base AS (
            SELECT
                DATE(published_at) AS d,
                COUNT(*) AS article_count,
                SUM( (title ILIKE '%%ai%%' OR content ILIKE '%%ai%%')::int ) AS ai_count,
                SUM( (title ILIKE '%%ë¨¸ì‹ ëŸ¬ë‹%%' OR content ILIKE '%%ë¨¸ì‹ ëŸ¬ë‹%%')::int ) AS ml_count,
                SUM( (title ILIKE '%%ì¸ê³µì§€ëŠ¥%%' OR content ILIKE '%%ì¸ê³µì§€ëŠ¥%%')::int ) AS ko_ai_count
            FROM news
            WHERE published_at IS NOT NULL
            GROUP BY 1
        )
        INSERT INTO news_daily_stats (stat_date, article_count, ai_count, ml_count, ko_ai_count, updated_at)
        SELECT d, article_count, ai_count, ml_count, ko_ai_count, now()
        FROM base
        ON CONFLICT (stat_date) DO UPDATE
        SET article_count = EXCLUDED.article_count,
            ai_count      = EXCLUDED.ai_count,
            ml_count      = EXCLUDED.ml_count,
            ko_ai_count   = EXCLUDED.ko_ai_count,
            updated_at    = now();
    """)

    conn.commit()
    cur.close()
    conn.close()


# DAG ì •ì˜
with DAG(
    "news_pipeline",
    default_args=default_args,
    description="ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ ì „ì²˜ë¦¬ â†’ CSV â†’ DB â†’ ì¼ë³„ ì§‘ê³„ (Slack ì•Œë¦¼ í¬í•¨)",
    schedule_interval= "*/10 * * * *",  # ìŠ¤ì¼€ì¤„ 
    start_date=datetime(2025, 9, 18),
    catchup=False,
    on_success_callback=dag_success_alert,
) as dag:

    t1 = PythonOperator(
        task_id="fetch_news",
        python_callable=fetch_news,
        provide_context=True,
    )

    t2 = PythonOperator(
        task_id="preprocess_news",
        python_callable=preprocess_news,
        provide_context=True,
    )

    t3 = PythonOperator(
        task_id="save_to_postgres",
        python_callable=save_to_postgres,
        provide_context=True,
    )

    t4 = PythonOperator(
        task_id="compute_daily_stats",
        python_callable=compute_daily_stats,
        provide_context=True,
    )

    t5 = PythonOperator(
    task_id="tokenize_news",
    python_callable=tokenize_news,
    provide_context=True,
    )


    t1 >> t2 >> t3 >> t4 >> t5
